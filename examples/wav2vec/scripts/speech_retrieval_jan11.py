import sys
import kaldiio
import torch

tsv = sys.argv[1]
xlsr_feats = sys.argv[2]


class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def calc_recalls(S):
    """
    Computes recall at 1, 5, and 10 given a similarity matrix S.
    By convention, rows of S are assumed to correspond to images and columns are captions.
    """
    assert(S.dim() == 2)
    assert(S.size(0) == S.size(1))
    if isinstance(S, torch.autograd.Variable):
        S = S.data
    n = S.size(0)
    A2I_scores, A2I_ind = S.topk(10, 0)
    I2A_scores, I2A_ind = S.topk(10, 1)
    A_r1 = AverageMeter()
    A_r5 = AverageMeter()
    A_r10 = AverageMeter()
    I_r1 = AverageMeter()
    I_r5 = AverageMeter()
    I_r10 = AverageMeter()
    for i in range(n):
        A_foundind = -1
        I_foundind = -1
        for ind in range(10):
            if A2I_ind[ind, i] == i:
                I_foundind = ind
            if I2A_ind[i, ind] == i:
                A_foundind = ind
        # do r1s
        if A_foundind == 0:
            A_r1.update(1)
        else:
            A_r1.update(0)
        if I_foundind == 0:
            I_r1.update(1)
        else:
            I_r1.update(0)
        # do r5s
        if A_foundind >= 0 and A_foundind < 5:
            A_r5.update(1)
        else:
            A_r5.update(0)
        if I_foundind >= 0 and I_foundind < 5:
            I_r5.update(1)
        else:
            I_r5.update(0)
        # do r10s
        if A_foundind >= 0 and A_foundind < 10:
            A_r10.update(1)
        else:
            A_r10.update(0)
        if I_foundind >= 0 and I_foundind < 10:
            I_r10.update(1)
        else:
            I_r10.update(0)

    recalls = {'A_r1':A_r1.avg, 'A_r5':A_r5.avg, 'A_r10':A_r10.avg,
                'I_r1':I_r1.avg, 'I_r5':I_r5.avg, 'I_r10':I_r10.avg}
                #'A_meanR':A_meanR.avg, 'I_meanR':I_meanR.avg}

    return recalls

text_embeds = []
audio_embeds = []
itr = 0
with open(tsv) as f1, open(xlsr_feats) as f2:
    for line1, line2 in zip(f1, f2):
        itr+=1
        splits_1 = line1.rstrip().split("\t")
        splits_2 = line2.rstrip().split()
        assert splits_1[0] == splits_2[0]
        speech_embedding = kaldiio.load_mat(splits_2[1])
        text_embedding = kaldiio.load_mat(splits_1[-1])[1]
        speech_embedding = torch.tensor(speech_embedding).float()
        text_embedding = torch.tensor(text_embedding).float()
        audio_embeds.append(speech_embedding)
        text_embeds.append(text_embedding)
        if itr % 200 == 0:
            print("Read %d" % itr)

audio_embeds = torch.stack(audio_embeds, dim=0)
text_embeds = torch.stack(text_embeds, dim=0)
S = torch.mm(text_embeds, audio_embeds.t())
recalls = calc_recalls(S)
print(recalls["A_r10"], recalls["I_r10"])
print(recalls["A_r5"], recalls["I_r5"])
print(recalls["A_r1"], recalls["I_r1"])

# python examples/wav2vec/scripts/speech_retrieval_jan11.py /gpfsscratch/rech/iqh/upp27cx/covost/manifests_norm_string/es_en/dev.tsv /gpfsscratch/rech/iqh/upp27cx/covost/feats/xlsr_300M_full_ft_cv_mls_mted_covost_mtedst_aishell/es_en/dev.xlsr.scp
# covost
# de es ca it ru zh-CN pt fa et mn nl tr ar sv-SE lv sl ta ja id cy
# fr_en: training with only CV_fr
# 0.9926666666666667 0.9893333333333333
# 0.9913333333333333 0.981
# 0.9716666666666667 0.9406666666666667

# de_en
# 0.9894900451483976 0.9846791503219599
# 0.9855673155206869 0.9801643105617645
# 0.9630671304862705 0.953445340833395

# es_en
# 0.9850994629755692 0.9823765221995311
# 0.9806368655926178 0.9746615233340897
# 0.9571136827773996 0.9436502533847667

# ca_en
# 0.8794972505891595 0.8695208169677926
# 0.8626080125687353 0.852395915161037
# 0.8062058130400629 0.7837391987431265

# it_en
# 0.974496644295302 0.9715883668903803
# 0.9708053691275168 0.9687919463087248
# 0.945413870246085 0.9398210290827741

# ru_en
# 0.8726677577741407 0.8150572831423896
# 0.837152209492635 0.7816693944353519
# 0.7332242225859247 0.6492635024549919

# zh-CN_en
# 0.3367747264092505 0.20730951889324797
# 0.2760685525500723 0.16436093330580218
# 0.16766467065868262 0.0858971711748916

# pt_en
# 0.9463532248342374 0.9309825195901146
# 0.9330922242314648 0.9144062688366486
# 0.8628691983122363 0.8474984930681133

# fa_en
# 0.6772133526850508 0.502467343976778
# 0.6133526850507982 0.4386066763425254
# 0.43541364296081275 0.2841799709724238

# et_en
# 0.47588832487309646 0.3350253807106599
# 0.4137055837563452 0.28426395939086296
# 0.28109137055837563 0.1814720812182741

# mn_en
# 0.07950028392958547 0.05167518455423055
# 0.06303236797274275 0.04031800113571834
# 0.029528676888131742 0.021578648495173196

# nl_en
# 0.9346674514420247 0.9181871689228959
# 0.912889935256033 0.9022954679223072
# 0.8281341965862272 0.8122424955856387

# tr_en
# 0.7438423645320197 0.6016009852216748
# 0.6859605911330049 0.5233990147783252
# 0.5160098522167488 0.3472906403940887

# ar_en
# 0.35494880546075086 0.32081911262798635
# 0.30716723549488056 0.2724687144482366
# 0.20648464163822525 0.17064846416382254

# ar_en : training with mgb2 only
# 0.674061433447099 0.6473265073947668
# 0.6137656427758816 0.573947667804323
# 0.431740614334471 0.4021615472127418

# sv-SE_en
# 0.3106004447739066 0.28020756115641215
# 0.25796886582653816 0.2357301704966642
# 0.15270570793180133 0.13046701260192736

# lv_en
# 0.2391111111111111 0.2
# 0.19466666666666665 0.15022222222222223
# 0.10844444444444444 0.08177777777777778

# sl_en
# 0.4656188605108055 0.3988212180746562
# 0.3791748526522593 0.3320235756385069
# 0.22789783889980353 0.1807465618860511

# ta_en
# 0.578125 0.6067708333333334
# 0.5260416666666666 0.5130208333333334
# 0.3671875 0.3411458333333333

# ja_en
# 0.09291338582677165 0.07716535433070866
# 0.06614173228346457 0.05511811023622047
# 0.026771653543307086 0.026771653543307086

# id_en
# 0.61489898989899 0.3952020202020202
# 0.5328282828282829 0.3371212121212121
# 0.3282828282828283 0.19570707070707072

# cy_en
# 0.20869565217391303 0.21884057971014492
# 0.18115942028985507 0.17681159420289855
# 0.10289855072463767 0.09855072463768116

# en_de
# 0.990856406954282 0.9887314874436575
# 0.9881519639407598 0.9855763039278815
# 0.9730199613650998 0.9678042498390212

# en_ca
# 0.990856406954282 0.986349001931745
# 0.98783000643915 0.9820991629104958
# 0.9735994848679974 0.9611075338055377

# en_zh-CN
# 0.9860914359304572 0.9770766258853831
# 0.9823567289117836 0.9717321313586607
# 0.9593045717965228 0.9435930457179652

# en_fa
# 0.9900193174500966 0.9840952994204765
# 0.9869285254346426 0.9791371538956858
# 0.9700579523502898 0.9542820347714102

# en_et
# 0.987894397939472 0.9845460399227302
# 0.9850611719253058 0.9815196394075982
# 0.9696072118480361 0.9634256278171281

# en_mn
# 0.9897617514488087 0.9861558274307791
# 0.9862202189311011 0.9796522858982615
# 0.9647134578235673 0.9511912427559562

# en_tr
# 0.9915647134578236 0.9889246619446234
# 0.9887958789439794 0.9846104314230522
# 0.9729555698647778 0.9656149388280747

# en_ar
# 0.9925949774629749 0.9819059884095299
# 0.9896973599484868 0.9765614938828074
# 0.9707662588538313 0.9462330972311654

# en_sv-SE
# 0.9901481004507405 0.9898261429491307
# 0.9866709594333548 0.9859626529298132
# 0.9710238248551192 0.9700579523502898

# en_lv
# 0.9900837089504185 0.989504185447521
# 0.9873148744365744 0.9853187379265936
# 0.9685125563425628 0.9643271088216355

# en_sl
# 0.9916934964584675 0.9889246619446234
# 0.9884739214423696 0.984996780424984
# 0.9740502253702511 0.9660656793303284

# en_ta
# 0.9951706374758532 0.9910495814552479
# 0.9934964584674822 0.987894397939472
# 0.9816484224082421 0.9691564713457823

# en_ja
# 0.9880231809401159 0.9810045074050225
# 0.9837733419188667 0.9759175788795879
# 0.9649710238248551 0.9468126207340631

# en_id
# 0.9917578879587894 0.9920154539600773
# 0.9886027044430136 0.989311010946555
# 0.9760463618802319 0.9749517063747586

# en_cy
# 0.9907276239536381 0.9877656149388281
# 0.9875724404378622 0.9841596909207985
# 0.9716033483580168 0.9661944623309723


